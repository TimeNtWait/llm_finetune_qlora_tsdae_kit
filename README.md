## llm_finetune_qlora_tsdae_kit
**LLM Domain Adapter Kit: Fine-Tuning LLMs with TSDAE + QLoRA**

Гибкий и эффективный инструмент дообучения языковых моделей под конкретные доменные задачи с использованием TSDAE и QLoRA.

### Описание
Этот проект - инструмент для дообучения открытых языковых моделей (доступных на Hugging Face) с помощью комбинации подходов **TSDAE (Transformer-based Denoising AutoEncoder)** и **QLoRA (Quantized LoRA)**.  
- **TSDAE** - дает качественную поднастройку под нужный домен.  
- **QLoRA** - дает максимальную скорость с минимальными затратами по мощностям.   

Результат — компактная и эффективная модель, адаптированная под необходимый домен данных: маркетплейсы, медицина, финансы, юриспруденция и др.

### Основные возможности
- Поддержка моделей `sentence-transformers`
- Дообучение с использованием **TSDAE** (обучение без разметки с использованием зашумления данных).
- Использование **QLoRA** для эффективного обучения, и при этома экономии памяти и ускорения обучения.
- Сохранение как полной модели, так и PEFT-адаптеров (для дальнейшего дообучения)
- Поддержка обучения на одной даже не самой мощной видеокарте (например, NVIDIA 3060)

## Обоснование выбора методов:
- Задача получить качественную модель с минимальными затратами на мощности
- **LoRA** - дообучение с помощью маленьких добавочных матриц (low-rank adapters), не трогая веса основной модели.
- **QLoRA** - это та же LoRA, только с более эффективным процессом fine tune за счет квантизации (4-bit) для экономии GPU памяти, что позволяет сделать дообучение даже на слабых мощностях.
- **TSDAE** - обучение эмбеддингов на неразмеченных данных за счет идеи зашумления данных.

#### QLoRA (Quantized LoRA)   
**QLoRA = Quantization + LoRA**  
Квантование — это техника уменьшения памяти (веса переводятся в int4, int8 и т.п.).  
LoRA — техника эффективного fine-tune.  
Их совместное использование = эффективная практика.  

Почитать про метод:   
- https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766
- [QLoRA: Efficient Finetuning of Quantized LLMs https://arxiv.org/abs/2305.14314](https://arxiv.org/abs/2305.14314) - оригинальная статья  

#### TSDAE (Transformer-based Denoising AutoEncoder)
TSDAE подход, при котором модель обучается восстанавливать исходный текст на основе его зашумлённой версии.

**Преимущества метода:**  
- Обучение без учителя, что не разметки данных и позволяет делать fine-tune имея только тексты. 
- Отлично подходит для адаптации модели под конкретный домен (в нашем случае — тексты и названия товаров).
- Эффективное подстраиваение под доммен даже с небольшим количеством обучающей выборки. 
- Повышает устойчивость модели к искажениям в текстах (опечатки, неверное написание и прочее).

**Как использовать:**  
- К каждому входному тексту применяется случайный шум (удаление слов, перестановки и прочее)
- Зашумлённый текст пропускается через энкодер.
- А декодер пытается восстановить исходную версию текста.
- В качестве функции потерь CrossEntropy между предсказанными и оригинальными токенами.
Таким образом, модель учится понимать структуру языка, контекст и семантику, создавая более качественные эмбеддинги.  

Почитать про метод:  
https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701/


**Итого:**   
TSDAE в сочетании с QLoRA позволяет эффективно обучить адаптационные LoRA-слои на больших объёмах даных используя ограниченные мощности.


```plaintext
.
├── src/
│   ├── config.py                # Общий конфиг дообучения
│   ├── create_dataset.py        # создание датасета (загрузка + предобработка)
│   ├── model_builder.py         # Формирование модели с QLoRA и TSDAE
│   ├── trainer.py               # Запуск обучения
│   ├── evaluation.py            # Метрики, сравнение, анализ
│   └── preprocess.py            # Пример скрипта для предобработки исходных данных     
├── requirements.txt             # Зависимости
├── README.md
└── .gitignore
```

Установка:  
Рекомендуется torch с поддержкой cuda ставить отдельно:  
```
pip3 install torch==2.6.0+cu126 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
```


#### Пример применения:  
[Fine-Tuning E5-small под домен русскоязычных описаний товаров маркетплейсов](README_e5_fine_tune-ru-products.md)    
**Базовая модель:** ["intfloat/multilingual-e5-small"](https://huggingface.co/intfloat/multilingual-e5-small)          
**Итоговая модель:** ["TimeNtWait/e5-small-ru-products-qlora_v1"](https://huggingface.co/TimeNtWait/e5-small-ru-products-qlora_v1)      
По итогам оценки на тестовых примерах заметно сильное улучшения качества для выбранного доменна данных. 

