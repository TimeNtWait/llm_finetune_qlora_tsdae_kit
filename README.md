## llm_finetune_qlora_tsdae_kit
**LLM Domain Adapter Kit: Fine-Tuning LLMs with TSDAE + QLoRA**

Гибкий и эффективный инструмент дообучения LLM-моделей под конкретные доменные задачи с использованием TSDAE и QLoRA.

### Описание
Этот проект - инструмент для дообучения открытых LLM-моделей (доступных на Hugging Face) с помощью комбинации подходов **TSDAE (Transformer-based Denoising AutoEncoder)** и **QLoRA (Quantized LoRA)**.  
- **TSDAE** - дает качественную поднастройку под нужный домен.  
- **QLoRA** - дает максимальную скорость с минимальными затратами по мощностям.   

Результат — компактная и эффективная модель, адаптированная под необходимый домен данных: маркетплейсы, медицина, финансы, юриспруденция и др.

### Основные возможности
- Поддержка моделей `sentence-transformers`
- Дообучение с использованием **TSDAE** (обучение без разметки с использованием зашумления данных).
- Использование **QLoRA** для эффективного обучения, и при этома экономии памяти и ускорения обучения.
- Сохранение как полной модели, так и PEFT-адаптеров (для дальнейшего дообучения)
- Поддержка обучения на одной даже не самой мощной видеокарте (например, NVIDIA 3060)

## Обоснование выбора методов:
- Задача получить качественную модель с минимальными затратами на мощности
- **LoRA** - дообучение с помощью маленьких добавочных матриц (low-rank adapters), не трогая веса основной модели.
- **QLoRA** - это расширенная версия LoRA, которая обеспечивает эффективный fine tune с квантизацией (4-bit) для экономии GPU памяти, что позволяет сделать дообучение даже на слабых мощностях.
- **TSDAE** - обучение эмбеддингов на неразмеченных данных за счет идеи зашумления данных.

#### QLoRA (Quantized LoRA) 
**QLoRA** — это расширенная версия LoRA, работающая за счёт квантования точности весовых параметров в предобученной LLM до 4-битной. Обычно параметры обученных моделей хранятся в 32-битном формате, но QLoRA сжимает их до 4-битного. Этот метод значительно уменьшает требуемый объём памяти, что позволяет запускать модели на менее мощном оборудовании.  

Почитать про метод:   
https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766

#### TSDAE (Transformer-based Denoising AutoEncoder)
TSDAE подход, при котором модель обучается восстанавливать исходный текст на основе его зашумлённой версии.

**Преимущества метода:**  
- Обучение без учителя, что не разметки данных и позволяет делать fine-tune имея только тексты. 
- Отлично подходит для адаптации модели под конкретный домен (в нашем случае — тексты и названия товаров).
- Эффективное подстраиваение под доммен даже с небольшим количеством обучающей выборки. 
- Повышает устойчивость модели к искажениям в текстах (опечатки, неверное написание и прочее).

**Как использовать:**  
- К каждому входному тексту применяется случайный шум (удаление слов, перестановки и прочее)
- Зашумлённый текст пропускается через энкодер.
- А декодер пытается восстановить исходную версию текста.
- В качестве функции потерь CrossEntropy между предсказанными и оригинальными токенами.
Таким образом, модель учится понимать структуру языка, контекст и семантику, создавая более качественные эмбеддинги.  

Почитать про метод:  
https://towardsdatascience.com/transforming-text-into-vectors-tsdaes-unsupervised-approach-to-enhanced-embeddings-728eb28ea701/


**Итого:**   
TSDAE в сочетании с QLoRA позволяет эффективно обучить адаптационные LoRA-слои на больших объёмах даных используя ограниченные мощности.


```plaintext
.
├── src/
│   ├── config.py                # Общий конфиг дообучения
│   ├── create_dataset.py        # создание датасета (загрузка + предобработка)
│   ├── model_builder.py         # Формирование модели с QLoRA и TSDAE
│   └── preprocess.py            # Пример скрипта для предобработки исходных данных     
├── requirements.txt             # Зависимости
├── README.md
└── .gitignore
```

Установка:  
Рекомендуется torch с поддержкой cuda ставить отдельно:  
```
pip3 install torch==2.6.0+cu126 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
```


